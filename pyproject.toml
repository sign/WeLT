[project]
name = "WeLT"
description = "WeLT: Word embedding Latent Transformer - a language model designed to operate on \"words\"."
version = "0.0.1"
authors = [
    { name = "Amit Moryossef", email = "amit@sign.mt" },
]
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "transformers[torch]<5",
    "trl", # For huggingface implementation of pack_dataset
    "datasets",
    "Pillow", # For AutoImageProcessor
    "torchvision", # For use_fast=True
    "cachetools", # For LRU cache in processor
    "vit-pytorch", # For NaViT model implementation
    # Our packages
    "words-segmentation>=0.0.5", # For pre-tokenization into words
    "utf8-tokenizer[fast]>=0.8.0", # For tokenization of bytes
    "pixel-renderer[pangocairo]>=0.1.4", # For text rendering into images (thread safe)
]

[project.optional-dependencies]
dev = [
    "ruff",
    "pytest",
    "pytest-xdist", # For parallel test execution
]

train = [
    "hf_transfer", # For HF_HUB_ENABLE_HF_TRANSFER
    "accelerate", # For distributed training
    "datasets", # For dataset loading and processing
    "evaluate", # For evaluation metrics
    "scikit-learn", # For "accuracy" metric in evaluate
    "sacrebleu", # For usual bleu/chrF metrics
    "wandb", # For experiment tracking
    "zstandard", # For compressing the data
]

dion = [
    "dion @ git+https://github.com/microsoft/dion.git", # For Dion2 optimizer
]

[tool.setuptools]
packages = [
    "welt",
    "welt.vision",
    "welt_training",
]

[tool.ruff]
line-length = 120
extend-exclude = [
    "welt_training/experiments/*",
]

[tool.ruff.lint]
select = [
    "E", # pycodestyle errors
    "W", # pycodestyle warnings
    "F", # pyflakes
    "C90", # mccabe complexity
    "I", # isort
    "N", # pep8-naming
    "UP", # pyupgrade
    "B", # flake8-bugbear
    "PT", # flake8-pytest-style
    "W605", # invalid escape sequence
    "BLE", # flake8-blind-except
]

[tool.pytest.ini_options]
addopts = "-v"
testpaths = [
    "welt",
    "tests",
]

[project.scripts]
welt-train = "welt_training.train:train"
welt-prepare-data = "welt_training.prepare_data:main"
