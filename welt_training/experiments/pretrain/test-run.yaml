# Test run config - quick sanity check for pretraining pipeline
# Usage: python -m welt_training.train welt_training/experiments/pretrain/test-run.yaml

# Model Architecture (from scratch - no pretrained weights)
model_name_or_path: null
load_pretrained: false

# Component models (small for fast testing)
image_encoder_model_name_or_path: null
# bytes_encoder_model_name_or_path: prajjwal1/bert-tiny
bytes_encoder_config: welt_training/experiments/pretrain/bert-tiny.json
latent_transformer_model_name_or_path: EleutherAI/pythia-70m
bytes_decoder_model_name_or_path: sbintuitions/tiny-lm

# Dataset - small subset
dataset_name: monology/pile-uncopyrighted

streaming: true
max_sequence_length: 128
max_word_length: 64
block_size: 128

# Data Loader (fewer workers for test)
dataloader_num_workers: 2
dataloader_prefetch_factor: 2
dataloader_pin_memory: true

# Training - minimal steps
remove_unused_columns: false
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
output_dir: ./output/test-run
do_train: true
max_steps: 1000
learning_rate: 3.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
weight_decay: 0.1
optim: adamw_torch_fused
seed: 42

# No checkpointing for test
save_strategy: "no"

# Evaluation - quick check
do_eval: true
eval_strategy: steps
eval_steps: 500
max_eval_samples: 16


# Logging
logging_steps: 10
include_tokens_per_second: true
include_num_input_tokens_seen: true
report_to: none

# Precision
bf16: true
dtype: bfloat16
