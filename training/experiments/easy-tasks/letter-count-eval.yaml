# Model Setup
image_encoder_model_name_or_path: null
bytes_encoder_model_name_or_path: prajjwal1/bert-tiny
latent_transformer_model_name_or_path: sbintuitions/tiny-lm
bytes_decoder_model_name_or_path: sbintuitions/tiny-lm
model_name_or_path: output/letter-count-small-cer

# Dataset setup
dataset_name: magus4450/english-words-small-letter-count
dataset_config_name: 
dataset_text_template: "<text>\x0E{text}\x0F<count> {count}"
max_sequence_length: 128
max_word_length: 32


max_eval_samples: 1000

# Data Loader
# dataloader_num_workers: 8
# dataloader_prefetch_factor: 4
# dataloader_pin_memory: true
# dataloader_persistent_workers: true

# Training setup
# remove_unused_columns: false # Necessary
# per_device_train_batch_size: 64
# per_device_eval_batch_size: 32
# auto_find_batch_size: true
# output_dir: ./output/letter-count-small-cer-test
# overwrite_output_dir: true
# do_train: true
# max_steps: 10
# learning_rate: 3.0e-4
# optim: adamw_torch_fused

# Evaluation
# do_eval: false
# eval_on_start: true
# eval_strategy: steps
# eval_steps: 10
# metric_for_best_model: accuracy


# EvaluationArguments
max_train_samples_for_eval: 64
max_eval_samples_for_eval: 64
max_test_samples_for_eval: 64
eval_metrics: ["accuracy", "cer"]

# Logging
# logging_steps: 10
# logging_strategy: steps
# include_tokens_per_second: true
# include_num_input_tokens_seen: true
# report_to: none

# Dtype
# bf16: true
dtype: bfloat16